{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Which linear regression training algorithm can you use if you have a training set with millions of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a training set with millions of features, stochastic gradient descent or mini-batch gradient descent are good choices.  But we can't use the normal equations or the SVD aproach because the computationl complexity grows quickly with the number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Suppose the features in our training set have very different scales.  Which algorithms might suffer from this, and how? What can we do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the features have different scales, cost functions form an elongated bowl, so gradient descent can take a long time to converge.  We can scale the data before training a model.  If we use the normal equations or SVD, we don't need to scale the features.  Regularized models may not converge to a suboptimal solution if the features are not scaled; since regularization penalizes large weights, features with smaller values will tend to be ignored compared to features with larger values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Can gradient descent get stuck in a local minimum when training a logistic regression model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No! It cannot because the cost function for logistic regression is convex, guaranteeing that the minimum is global.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Do all gradient descent algorithms lead to the same result if we let them run long enough? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the optimization problem is convex (linear regression, logistic regression, etc.) and assuming we don't make the learning rate too high, then all gradient descent algorithms approach a global optimum and end up producing mostly similar results.  However, in the case of SGD and MBGD, unless we reduce the learning rate, they never truly converge; they jump around the global optimum.  So even if we let them run for a long tme, they produce slightly different outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Suppose you use batch gradient descent and plot the validation error at every epoch.  If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the validation error consistently goes up after every epoch, one possibility is that the learning rate is too high and the algorithm is diverging.  If the training error also goes up, this is clearly the problem and we should reduce the learning rate.  However, if the training error is not going up, then our model is overfitting the training set and we should stop training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Is it a good idea to stop mini-batch gradient descent immediately when the validation error goes up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No! Since they are random, SGD and MBGD are not guaranteed to make progress at every single training iteration.  If we immediately stop the training when the validation error goes up, we may stop too early before the optimum is reached.  A better option is to save the model at regular intervals; when it has not improved for a long time, we can revert tot he best saved model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Which gradient descent algorithm will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can we make the others also converge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent has the fastest training iteration since it considers only one training instance at a time, so it is generally the first to reach the vicinity of the global optimum.  However, only batch gradient descent will actually converge, given enough training time.  SGD and MBGD will bounce around the optimum unless we gradually reduce the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Suppose we are using polynomial regression.  We plot the learning curves and notice that there is a large gap between the training error and the validation error.  What is happening? What are three ways to solve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the validation error is much higher than the training error, it is likely because the model is overfitting on the training set.  To fix this, we could reduce the polynomial degree; a model with fewer degrees of freedom is less likely to overfit.  We could also try to regularize the model by adding an l2 or l1 penalty to the cost function.  This will also reduce the degrees of freedom of the model.  Lastly, we can try to increase the size of the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Suppose you are using ridge regression and notice that the training error and the validation error are almost equal and fairly high.  Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter $\\alpha$ or reduce it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is suffering from high bias since the model is not learning the training data well.  We could attempt to reduce this by reducing the value of alpha, thus decreasing the severity of the regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Why would you want to use:\n",
    "\n",
    "- a. Ridge regression instead of vanilla linear regression? \n",
    "- b. Lasso instead of ridge? \n",
    "- c. Elasticnet instead of lasso? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. A model with some regularization typically performs better than a model without any regularization, so we should generally prefer ridge over plain linear regression.\n",
    "\n",
    "b. Lasso uses an l1 penalty, which tends to push weights down to exactly zero.   This leads to sparse models, where all weights are zero except for the most important.  This is a way to perform feature selection automatically, which is good if we suspect that only a few features actually matter.  When we are not sure, we should prefer to use ridge regression. \n",
    "\n",
    "c. Elastic Net is generally preferred over lasso since lasso may behave erratically in some cases, such as when features are strongly correlated or there are more features than training instances.  However, it adds an extra hyperparameter to tune.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Suppose we want to classify pictures as outdoor/indoor and daytime/nighttime.  Should we implement two logistic regression classifiers or one softmax regression classifier? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to classify pictures as outdoor/indoor and daytime/nighttime, since these are not two exclusive classes in the sense that any of these four combinations are possible, we should train two logistic regression classifiers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
